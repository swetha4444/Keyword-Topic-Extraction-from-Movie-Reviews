{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd095c10dbc6f7eccef0c1ace84822d618f7863d3bc26cab307fc0169bb43c23fbe",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import pyprind\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To change date to datetime\n",
    "from datetime import datetime\n",
    "import re \n",
    "\n",
    "from collections import Counter\n",
    "import string\n",
    "import scipy.sparse\n",
    "\n",
    "# Gensim libraries\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import matutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis"
   ]
  },
  {
   "source": [
    "## Read data and convert thr reviews to list"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('clean_reviews.csv')\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract lematized words length not less than 2\n",
    "def minimize_review(text): # text\n",
    "    rev_text = ([token for token in text.split(' ') if len(token) > 3])\n",
    "    return rev_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_reviews = []\n",
    "for review in dataframe.processed_text:\n",
    "  clean_reviews.append(minimize_review(review))\n",
    "len(clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create Dictionary\n",
    "id2word_1 = corpora.Dictionary(clean_reviews)\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus_1 = [id2word_1.doc2bow(review) for review in clean_reviews]\n",
    " # Build LDA model\n",
    "ldamodel = LdaMulticore(corpus= corpus_1, num_topics =8, id2word=id2word_1,chunksize=2000, passes=10,per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(ldamodel.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "#It's a measure of how good the model is. The lower the better. Perplexity is a negative value\n",
    "print('\\nPerplexity: ', ldamodel.log_perplexity(corpus_1))  \n",
    "\n",
    "# Compute Coherence Score\n",
    "# The coherence score is used in assessing the quality of the learned topics, the closer to 1 the better\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts=clean_reviews, dictionary=id2word_1, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nBasic Ldamodel Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "source": [
    "## Choosing the optimal number of topics to optimize coherence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = []\n",
    "coherence = []\n",
    "model_list = []\n",
    "for i in range(4,15,2):\n",
    "    print(\"Computing for topics = \",i)\n",
    "    ldamodel = LdaMulticore(corpus= corpus_1, num_topics =i, id2word=id2word_1,chunksize=2000, passes=30,per_word_topics=True)\n",
    "    model_list.append(ldamodel)\n",
    "    coherence_model_lda = CoherenceModel(model=ldamodel, texts=clean_reviews, dictionary=id2word_1, coherence='c_v')\n",
    "    num_topics.append(i)\n",
    "    coherence.append(coherence_model_lda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_topics, coherence)\n",
    "plt.title(\"Coherence value score with the number of topics\")\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the best topics\n",
    "optimal_model = model_list[1]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the topic\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_prepared = pyLDAvis.gensim_models.prepare(ldamodel, corpus=corpus_1, dictionary=id2word_1,sort_topics=False)\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(LDAvis_prepared, 'LdaModel_viz.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}